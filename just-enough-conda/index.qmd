---
title: Just enough Conda advice
subtitle: Easy enough for beginners, good enough for seasoned users
summary: The best way to make things "just work" is for to design your workflow so that common mistakes are simply impossible.
author: Michael DeCrescenzo
date: 2025-04-05
---

Virtual environments seem to be a constant pain for researchers.
Academics in particular seem to not really understand them, which causes a lot of startup pain when they want try out the Python language.
The academics I used to roll with are heavy users of R, which has some environment management tools (`renv` in particular), but they are not heavily used.[^cran]
Even in the industry world I see Python users complaining about environment pains.
Stuff like, "broke my conda environment again...".

[^cran]: Most R users survive on a combination of CRAN's blessings and hope, which is another way of saying, complete avoidance of env tooling and workflow.]

This post will be a rapid-fire list of env-management practices that should ease a lot of these pains.

I will focus on the `conda` package manager tool because Conda can do multi-language environment configuration,[^mamba] whereas `pip` and `uv` are primarily focused on Python-only dependency management.[^r-conda]
But the principles discussed here generally apply to any major Python package manager.

[^r-conda]: For example, you can manage R environments with Conda, which I do.

[^mamba]: I personally use [micromamba](https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html), which behaves basically like Conda but is smaller and faster.

This post may evolve as I evolve my thoughts about what good practices are.
But I have been using conda in a professional environment for 5 years now, and I think the information here is both:

- good enough that experienced users can endorse it.
- efficient enough that total newcomers can adopt 100% of these tips and feel like none of the effort was wasted.

If you are familiar with conda environments already, feel free to skip around and get what you want out of this.
If you are totally new, reading in order will help you get onboarded.


## Managing the environment is easy when you are consistent about it

In his Statistical Rethinking lectures, Richard McElreath says that statistical machinery helps us solve problems [not because we are clever, but because we are rigorous](https://www.youtube.com/watch?v=lTFAB6QmwHM&) in the application of statistical fundamentals.

Similarly, environments help you solve problems when they are applied rigorously.

Many people first learn about environments and want to minimize their interactions with the environment.
They want things to "just work", which is understandable, but they think they can achieve this by hiding the environment.
They want the environment to be an invisible background consideration, an inconvenience whose existence should be minimized.

Sorry, but this is all wrong.
The environment is saving you from a ton of pain, and you should embrace it.
The best way to make things "just work" is for to design your workflow so that common mistakes are simply impossible.

This means you have to learn.


## What is an environment?

An environment is like a "scope" that defines which computational resources are available and how programs can find those resources.

Stepping back.
Your computer can contain many software tools on it, and those tools "live" in certain places in the computer.
When you run some piece of software, and it needs to reach out to some other thing you have installed on your computer (such as a matrix multiplication library), how does the software know where to find that other program?
And are these programs compatible with each other?

Environments are one way to manage which dependencies are available to your program.

It is helpful to see this working in the terminal with an example.

This current blog post is written in Quarto, and I use the Quarto CLI to render the post.
If I open a terminal and ask if the `quarto` tool exists, the terminal shows me where it is finding that tool.

```sh
which quarto

# /usr/local/bin/quarto

quarto --version

# 1.6.40
```

This is saying that I have a `quarto` version 1.6.40 executable in the `/usr/local/bin` directory.

That's nice.
But I actually don't use this instance of `quarto` to build this post.
I have a virtual environment for this post managed by [`pixi`](https://pixi.sh/latest/) (which is built atop Conda).
This virtual environment has its own installation of `quarto`, which I keep because I don't want any changes to the global `quarto` that I install on my computer to break how I build _this specific blog post_.
So I render this post _in this virtual environment_, and the environment will use the specific version of `quarto` that I lock down in this environment.

```sh
pixi run which quarto

# /Users/michaeldecrescenzo/projects/site-hugo/submodules/blog-monorepo/just-enough-conda/.pixi/envs/default/bin/quarto

pixi run quarto --version

# 1.7.33
```

Every time I say `pixi run ...` I am running the following command in the environment that `pixi` governs.
And because this environment is its own isolated thing inside my computer, the `quarto` executable in that environment is not the one in my computer's "default" scope.

**tldr** An environment defines an ecosystem of dependencies that can be configured for _joint compatibility_, so that your software can be built and executed in a consistent and enduring way.


## Why you should use environments

The benefit of this is you can control many separate projects on your computer that have different computational requirements.

- Your projects need different versions of R or Python, depending on the project.
- One project needs some package, but another project doesn't need it.
- You want to prevent software upgrades from breaking your code, so you want to specify _which_ version of a package you need for this project.
- You want to test your project on another computer, and you need a way to quickly install the required dependencies on that other computer.
- You move your project to a different location on _your own_ machine, and you need to make sure all of the dependencies are in the right place.

Environments save you from all of this.

But you _must_ use them intentionally to reap these benefits.


## Start in the terminal

Although environments can be invoked by various GUI applications like VSCode or RStudio, I think there is no substitute for learning the fundamentals in the shell.
The terminal is where you learn what environments actually are, and if it doesn't make sense in the terminal, it is unlikely to make sense in the GUI.
GUIs can obscure what is really going on, they can set _their own_ environment variables that interfere with the behaviors that you think you should be getting out of the environment.
It can be a huge mess, and if you don't know what _should_ be happening (wisdom that you gain in the terminal), then you will have a hard time diagnosing how the GUI application is messing with you.

This doesn't mean you only have to use terminal-based applications like vim to do your work.
Instead, I recommend launching GUI applications _from the correct environment_ in order to inherit your environment's behaviors.

For example, if I just open my terminal emulator in my computer (no special environment active), I can open VScode from the terminal like this:

```sh
# open VSCode focusing on the current folder '.'
code .
```

...And VSCode is opened subject to the environment variables in that shell.
If I open VSCode's terminal and ask what `python` program is available, I get whatever I would have gotten in the shell that launched VSCode.
As it happens, I don't have a Python install available from that context.

```sh
which python
# python not found
```

But if I go to my website repo's environment (which is managed by `uv`) and then launch VSCode from _that_ environment...

```sh
cd projects/site-hugo

# run "code ." in the local uv environment
uv run code .
```

then VSCode inherits that environment's background behaviors.
Which means that the VSCode terminal is aware of the Python binary that I use to managing my website.

```sh
which python
# /Users/michaeldecrescenzo/projects/site-hugo/.venv/bin/python
```

So it is possible to both (a) get comfortable managing environments in the terminal, and (b) use the same GUI apps to edit your code as before.


## Projects should never share environments

Many people start using environments by building a single conda environment that they can activate from anywhere.
This is a recipe for disaster, because different projects may want competing dependencies.

This is one reason why you see "broke my conda environment again..." tweets.
They may have been using the same environment for too many things.

You should have one environment per project.
Or more!
Some projects require multiple environments.

But you should never mix multiple projects into one environment.


## Build environments into a local path

When you create a conda environment, keep it inside the project directory.
You can do this with 

```sh
conda create -p path/to/env <list packages here>
```

or if your environment is defined in a `yml` file (more on that later):

```sh
conda env create -f path/to/file.yml -p path/to/env
```

In both examples, the `-p path/to/env` argument says to build and store the environment at that path, instead of in some global "default" path that conda/mambda/micromamba sets up for you.

What will actually live at that path is a bunch of folders and files containing binaries, packages, and so on that your project will use.
Quite literally, you will install Python/R/whatever you need into that folder.

The upside to this practice is that it's always easy to associate your project with its correct environment.
When you have environments floating around in the default global path, it is easy to forget which environment goes with which project.
It is needless mental burden.

You may find it helpful to mark these env paths as "private" directories however you wish (and add them to `.gitignore`).
Some people like to put these environments in a `.venv` directory (the `.` at the front of the name hides the directory) or in directories whose names are prefixed with an underscore.
I often do `_build/envs/env-name`:

- `_build` contains many things that I might build, which includes environments but also other things like generated data or output of some other kind
- `envs` contains any environments, obviously
- `env-name` is the specific name of the environment, but the project might need another environment to handle a separate, specific thing.
  This can happen if you need to compile something in one environment but use it in another environment.
  Situations like this are pretty common!


## You can run commands in the environment without "activating" the environment

"Activating" the environment means to initialize your terminal shell with paths and environment variables that the environment defines.
When the env is "active" then all commands will be run in that environment, without you specifying anything extra.
So `which python` would give me a different answer after I activate an environment that contains a Python installation.

But you don't have to activate the environment in order to run commands inside the environment.
With conda/mamba/micromamba for example you can do `micromamba run -p path/to/env <insert cmd here>` in order to run the command in the environment defined at `path/to/env`.[^uv-run]
This is helpful when writing scripts: you can explicitly invoke an environment for specific commands without relying on any background context.
We saw examples of this above already.

[^uv-run]: or `uv run <cmd>`, or `pixi run <cmd>`, and so on.

This is useful in scenarios where you have multiple environments in play.
For example, when I build my website, I also want to render any blog posts that may have changed, but those blog posts have their own computing environments (some use R code, some use Python code, some use none...).
When I render my blog posts, I need to do a series of `micromamba run -p path/to/blog-specific-env quarto render path/to/specific-blog-post.qmd` to render each post in its matching environment.
This may sound tedious at first, but

- It is necessary to prevent the dependencies of multiple blog posts from clashing with one another.
- it is actually "easy", because the blog posts and their environments are associated to each other in a config file.

So all the difficulty of the coordinated environments can be handled programmatically as long as you have done the proper organizational work.
This is why it pays to be rigorous!
When your system follows good rules, you can leverage the computer to do the correct thing, easily, even in complex situations.


## Never use a global `.condarc` file

A [`.condarc`](https://docs.conda.io/projects/conda/en/latest/user-guide/configuration/use-condarc.html) defines some "common" behavior for all conda/mamba operations on your machine.
You can define some "default" channels that you want to search for packages to install and configure some options for how the dependency resolver algorithm works.

You should not use this file, because it controls _your machine_ only.
This undermines reproducibility by injecting hidden configuration that is not checked into your project's source code control.
It is a big mistake that creates tons of reproducibility problems once you start sharing code with other people.
_Do not ever use this file_, and you should encourage your coworkers to stop using it also.

The way to get around `.condarc` is to be explicit when creating/modifying environments about which "channels" you are requesting packages from _at the level of the project_.

This sounds tedious, but it won't be if you also _define your environment in a `.yml` file_.


## Build your conda environment from a `.yml`. Never install things as you go

Another major footgun is to install things into your environment on an ad-hoc basis, for example `conda install polars`.[^dep]
Package managers are nice because they can find a version of a package that can be installed into your present environment.
But things go wrong.
Sometimes they install a package into your environment while uninstalling or changing the versions of other dependencies, so it can break your code.
There are also sometimes situations where the package versions you get with one single environment "solve" are different from the versions you get with post-hoc package installations.
This means your enviornment is no longer reproducible.

Instead: define a `yml` file that lists the channels and packages to install, and build the environment from that file.
You can pin the package versions you need, which gives you more control over how your environment will be created in the future.
Here is an [example yml file from my website repo](https://github.com/mikedecr/site-hugo/blob/main/conda/hugosite.yml):

```yml
name: hugosite
channels:
  - conda-forge
  - r
  - nodefaults

dependencies:
  # website core dependencies
  - hugo == 0.138.0
  - quarto == 1.5.57
  # scripting
  - python >= 3.10
  - ipython
  - typer
  - setuptools
```

And you would create this environent (according to all above instrucions) like this:

```sh
micromamba create -f path/to/env-recipe.yml -p path/to/built-env
```

...which says "build the environment defined in this file, and put it at this path".

If you don't use conda, other package managers have other ways of encoding dependencies into a file for reproducibility, such as `requirements.txt` or `pyproject.toml`.
Regardless of your choice of tool, you should do something like this.

[^dep]: Didn't you hear? Pandas is deprecated.


## Pin your dependencies

If you have read this far, it should not be a huge surprise that you should pin your dependencies to the versions that you expect to consume in your project.
If you know that you need some minimum version of a package to pick up a new feature, specify that in your environment recipe.

This can be done to pin the maximum version of a package as well, in case new versions broke a feature that you needed to work.

If you see someone complaining that they updated a package and now their code doesn't work, they didn't do this part right.


## Other quick tips:

This stuff is more optional, but can improve your UX.

- [**Direnv**](https://direnv.net/). In the simple case where you want to activate an environment when you are in a project directory, you can use Direnv.
  This creates the effect where you `cd` to the directory, and your environment is automatically activated.
- **R environments**.
  You can manage R projects with Conda/Mamba/Micromamba just fine.
  You can ask for packages prefixed with `r-` from the `r` conda channel.
  Example from my own blog posts [here](https://github.com/mikedecr/post_memo_scum/blob/c9af2771a61ef483d7335c3f41a5a309cf832e6b/conda/env.yml).
  I think this is way easier and more predictable than `renv`, which has never worked the way I want it to.
- **Makefiles and friends**: If you have other setup scripts that need to run that cannot be entirely managed by your environment / package manager, systematizing how those commands should be run is also very helpful.
  Makefiles are one way to do it.
  I prefer [Just](https://github.com/casey/just), which I find far easier to write.
  You can install `just` with conda or pip.
- **UV, Pixi, etc**.
  People realy like [uv](https://docs.astral.sh/uv/), a Python package manager written in Rust.
  I love `uv` when it can support my projects, but because `uv` focuses on the Python Package Index, it isn't a good choice for projects that need e.g. C/C++ dependencies, or R projects.
  So that's why we talked about Conda and friends here.
  If you like the speed and feel of `uv`, but you need a more cross-language package index, [`Pixi`](https://pixi.sh/latest/) has been fun to use.
